{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "003_machine_learning_with_scikit_learn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LCQ5IM2Nt7EU"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-analytics-workshop/python/blob/master/003_machine_learning_with_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5JsOY7DWVS7",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning with Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGRZ7KBydODb",
        "colab_type": "text"
      },
      "source": [
        "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-daFkl9V7H8g",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn is a free machine learning library for Python. It features various algorithms like support vector machine, random forests, and k-neighbours, and it also supports Python numerical and scientific libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuMahuoTWYKI",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc1dFIvAVkgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library for Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chFV2XFghGsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library for Visualization\n",
        "import matplotlib. pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRCtj0Udx90",
        "colab_type": "text"
      },
      "source": [
        "## **Supervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqyFFdRhfK4M",
        "colab_type": "text"
      },
      "source": [
        "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBVWQUCr3SkU",
        "colab_type": "text"
      },
      "source": [
        "### **1. Regression Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhMF_SSPe8nn",
        "colab_type": "text"
      },
      "source": [
        "Regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzvzIks4hNit",
        "colab_type": "text"
      },
      "source": [
        "#### ***a. Linear Regression***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhfHAEQbevjb",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tji-5BawhdOP",
        "colab_type": "text"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGhjqZojhfKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Dataset\n",
        "df_salary = pd.read_csv('https://raw.githubusercontent.com/andrybrew/all-files/master/dataset_python/salary_data.csv')\n",
        "df_salary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TIZx9mahpEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints the Dataset Information\n",
        "df_salary.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCA765vHw8Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints Descriptive Statistics\n",
        "df_salary.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUweVEqxNxX",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEN9JkjjxR4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Linear Regression Module\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Modeling Linear Regression\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Select X and Y Variable\n",
        "X = df_salary.iloc[:, :-1].values\n",
        "Y = df_salary.iloc[:, 1].values\n",
        "\n",
        "# Apply Model to Data\n",
        "lr.fit(X, Y)\n",
        "\n",
        "# Show Coefficent and Intercept\n",
        "print('Coefficient = ', lr.coef_)\n",
        "print('Intercept =', lr.intercept_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlLFIn5cx5La",
        "colab_type": "text"
      },
      "source": [
        "**Visualizing Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHS7aAeXx7hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Draw Scatter Plot with X and Y Axes\n",
        "plt.scatter(X, Y)\n",
        "\n",
        "# Show Regression Line with Green Color\n",
        "plt.plot(X, lr.predict(X), color = 'green')\n",
        "\n",
        "# Set Title and Axes Name\n",
        "plt.title('Salary vs Experience')\n",
        "plt.xlabel('Years of Experience')\n",
        "plt.ylabel('Salary')\n",
        "\n",
        "# Show Plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxlIGYbZydFR",
        "colab_type": "text"
      },
      "source": [
        "#### ***b. Logistic Regression Model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naQu8ytmjV3s",
        "colab_type": "text"
      },
      "source": [
        "logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8CmJkUsTzeYU"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCEspOFVzeYV",
        "colab": {}
      },
      "source": [
        "# Import Dataset\n",
        "df_churn = pd.read_csv('https://raw.githubusercontent.com/dianrdn/data/master/customer_churn.csv', sep = ';')\n",
        "df_churn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B6o_x1hDzeYZ",
        "colab": {}
      },
      "source": [
        "# Prints the Dataset Information\n",
        "df_churn.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A-SxL5OwzeYc",
        "colab": {}
      },
      "source": [
        "# Prints Descriptive Statistics\n",
        "df_churn.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OympUMG0sBV",
        "colab_type": "text"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf6TfMDDwBee",
        "colab_type": "text"
      },
      "source": [
        "Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlWqQPW_wDJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for Missing Values\n",
        "df_churn.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urZMnUO1wGmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Search for Median Value\n",
        "median = df_churn['TotalCharges'].median()\n",
        "\n",
        "# Use Median to Replace Missing Values\n",
        "df_churn['TotalCharges'].fillna(median, inplace=True)\n",
        "\n",
        "# Check for Missing Values\n",
        "df_churn.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxGx6X8zwTVG",
        "colab_type": "text"
      },
      "source": [
        "Encode Categorical Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOprAnLcwWWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Encode Categorical Data\n",
        "df_encoded = pd.DataFrame(encoder.fit_transform(df_churn[['gender', 'InternetService', 'Contract', 'PaymentMethod']]))\n",
        "df_encoded.columns = encoder.get_feature_names(['gender', 'InternetService', 'Contract', 'PaymentMethod'])\n",
        "\n",
        "# Replace Categotical Data with Encoded Data\n",
        "df_churn.drop(['gender', 'InternetService', 'Contract', 'PaymentMethod'] ,axis=1, inplace=True)\n",
        "df_churn_encoded= pd.concat([df_churn, df_encoded], axis=1)\n",
        "\n",
        "# Show Encoded Dataframe\n",
        "df_churn_encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm__QD8SxBqc",
        "colab_type": "text"
      },
      "source": [
        "Set Feature and Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQFTig_v0klf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Features\n",
        "feature = df_churn_encoded.drop(['customerID', 'Churn'], axis=1)\n",
        "feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vacgNWxh0q_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Target\n",
        "target = df_churn_encoded['Churn']\n",
        "target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxni4q9HxU_h",
        "colab_type": "text"
      },
      "source": [
        "Set Training and Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcS1t1Rp020y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Training and Testing Data (70:30)\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "feature_train, feature_test, target_train, target_test = train_test_split(feature , target, shuffle = True, test_size=0.3, random_state=1)\n",
        "\n",
        "# Show the Training and Testing Data\n",
        "print(feature_train.shape)\n",
        "print(feature_test.shape)\n",
        "print(target_train.shape)\n",
        "print(target_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgYv93ja09sJ",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_V1n5hA1Hir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Modeling Logistic Regression\n",
        "logreg = LogisticRegression()\n",
        "logreg_model = logreg.fit(feature_train, target_train)\n",
        "\n",
        "# Predict Test Data \n",
        "target_predicted_logreg = logreg.predict(feature_test)\n",
        "target_predicted_logreg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tavjnJRa2m4u",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNkLvigV2qUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# Confsion Matrix\n",
        "cm_logreg = metrics.confusion_matrix(target_test, target_predicted_logreg)\n",
        "cm_logreg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f10omVr72wa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy, Precision, Recall\n",
        "acc_logreg = metrics.accuracy_score(target_test, target_predicted_logreg)\n",
        "prec_logreg = metrics.precision_score(target_test, target_predicted_logreg)\n",
        "rec_logreg = metrics.recall_score(target_test, target_predicted_logreg)\n",
        "f1_logreg = metrics.f1_score(target_test, target_predicted_logreg)\n",
        "kappa_logreg = metrics.cohen_kappa_score(target_test, target_predicted_logreg)\n",
        "\n",
        "# Show Accuracy, Precision, Recall\n",
        "print('Accuracy:', acc_logreg)\n",
        "print('Precision:', prec_logreg)\n",
        "print('Recall:', rec_logreg)\n",
        "print('F1 Score:', f1_logreg)\n",
        "print('Cohens Kappa Score:', kappa_logreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAhgZWteyx-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Visualization Package\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Visualize ROC Curve\n",
        "target_predicted_logreg_prob = logreg.predict_proba(feature_test)[::,1]\n",
        "fp_rate_logreg, tp_rate_logreg, _ = metrics.roc_curve(target_test,  target_predicted_logreg_prob)\n",
        "auc_logreg = metrics.roc_auc_score(target_test, target_predicted_logreg_prob)\n",
        "plt.plot(fp_rate_logreg, tp_rate_logreg, label='Decision Tree, auc='+str(auc_logreg))\n",
        "plt.xlabel('false positive rate') \n",
        "plt.ylabel('true positive rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLdOajMO3_PL",
        "colab_type": "text"
      },
      "source": [
        "### **2. Classification Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxW-hOrgmQ2r",
        "colab_type": "text"
      },
      "source": [
        " classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwnZdZ3mO0k",
        "colab_type": "text"
      },
      "source": [
        "We use churn dataframe that already created before, including all of the data preparation steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9mNT4CUu4_Yw"
      },
      "source": [
        "**Show Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_RxOGxpt4_Yx",
        "colab": {}
      },
      "source": [
        "# Show Raw Dataset\n",
        "df_churn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHsAYRbh1GA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Encoded Dataframe\n",
        "df_churn_encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9PDKadJ1JE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Features\n",
        "feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqWTxIwa1M01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Targets\n",
        "target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og5PrN9e1X0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show the Training and Testing Data\n",
        "print(feature_train.shape)\n",
        "print(feature_test.shape)\n",
        "print(target_train.shape)\n",
        "print(target_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpxlBH9W3BE2",
        "colab_type": "text"
      },
      "source": [
        "##### ***a. Decision Tree Classifier***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgOX-StZnMvN",
        "colab_type": "text"
      },
      "source": [
        "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_jYRdpBotGt",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5olHo38B4_ZR",
        "colab": {}
      },
      "source": [
        "# Import library\n",
        "from sklearn import tree\n",
        "\n",
        "# Modeling Decision Tree\n",
        "dtree = tree.DecisionTreeClassifier(min_impurity_decrease=0.01)\n",
        "dtree.fit(feature_train, target_train)\n",
        "\n",
        "# Predict Test Data \n",
        "target_predicted_dtree = dtree.predict(feature_test)\n",
        "target_predicted_dtree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtpTZzrT15gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize Tree\n",
        "\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(dtree, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,\n",
        "                class_names=['notchurn', 'churn'],\n",
        "                feature_names=['SeniorCitizen',\t'Partner',\t'Dependents', 'tenure',\t'PhoneService', 'OnlineSecurity',\t'OnlineBackup',\t'DeviceProtection',\n",
        "                               'TechSupport',\t'StreamingTV',\t'StreamingMovies',\t'PaperlessBilling',\t'MonthlyCharges', 'TotalCharges', 'gender_Female',\n",
        "                               'gender_Male',\t'InternetService_DSL', 'InternetService_Fiber optic', 'InternetService_No',\t'Contract_Month-to-month',\n",
        "                               'Contract_One year',\t'Contract_Two year',\t'PaymentMethod_Bank transfer (automatic)', 'PaymentMethod_Credit card (automatic)',\n",
        "                               'PaymentMethod_Electronic check',\t'PaymentMethod_Mailed check'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "Image(graph.create_png())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzO27B92pKtD",
        "colab_type": "text"
      },
      "source": [
        "In the decision tree chart, each internal node has a decision rule that splits the data. Gini referred as Gini ratio, which measures the impurity of the node. You can say a node is pure (gini = 0) when all of its records belong to the same class, such nodes known as the leaf node.\n",
        "\n",
        "Samples is the sum of data train which actually belongs to the decision. The first node samples is equal to the whole data train number = 4930.\n",
        "\n",
        "Value is the sum of data train which is predicted belong to each of next node. For the first node, the values are [3589, 1341]. This means that 3589 data are predicted as notchurn, while 1341 data are predicted as churn.\n",
        "\n",
        "Class is stated the class categori. The class of the first node is 'notchurn'. So, the next step is, if the a data have a class of notchurn = true, we move to the left node, and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhxeRbuO1w9X",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7m2XcUlQ4_ZV",
        "colab": {}
      },
      "source": [
        "# Confsion Matrix\n",
        "cm_dtree = metrics.confusion_matrix(target_test, target_predicted_dtree)\n",
        "cm_dtree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuJaroTA33hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy, Precision, Recall\n",
        "acc_dtree = metrics.accuracy_score(target_test, target_predicted_dtree)\n",
        "prec_dtree = metrics.precision_score(target_test, target_predicted_dtree)\n",
        "rec_dtree = metrics.recall_score(target_test, target_predicted_dtree)\n",
        "f1_dtree = metrics.f1_score(target_test, target_predicted_dtree)\n",
        "kappa_dtree = metrics.cohen_kappa_score(target_test, target_predicted_dtree)\n",
        "\n",
        "# Show Accuracy, Precision, Recall\n",
        "print('Accuracy:', acc_dtree )\n",
        "print('Precision:', prec_dtree)\n",
        "print('Recall:', rec_dtree)\n",
        "print('F1 Score:', f1_dtree)\n",
        "print('Cohens Kappa Score:', kappa_dtree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCWRRoAh26gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Visualization Package\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Visualize ROC Curve\n",
        "target_predicted_dtree_prob = dtree.predict_proba(feature_test)[::,1]\n",
        "fp_rate_dtree, tp_rate_dtree, _ = metrics.roc_curve(target_test,  target_predicted_dtree_prob)\n",
        "auc_dtree = metrics.roc_auc_score(target_test, target_predicted_dtree_prob)\n",
        "plt.plot(fp_rate_dtree, tp_rate_dtree, label=\"Decision Tree, auc=\"+str(auc_dtree))\n",
        "plt.xlabel('false positive rate') \n",
        "plt.ylabel('true positive rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rsMuJbxXDOsN"
      },
      "source": [
        "##### ***b. K-Nearest Neighbor***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUgxDmicsqJc",
        "colab_type": "text"
      },
      "source": [
        "K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwHtNjopo1YJ",
        "colab_type": "text"
      },
      "source": [
        "**Modeling K-Nearest Neighbor Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoyKZkCJDOsR",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Modeling Naive Bayes\n",
        "knn = KNeighborsClassifier(n_neighbors= 71)\n",
        "knn.fit(feature_train, target_train)\n",
        "\n",
        "# Predict Test Data \n",
        "target_predicted_knn = knn.predict(feature_test)\n",
        "target_predicted_knn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7oGMwk6o7x6",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uTQodPgXDOse",
        "colab": {}
      },
      "source": [
        "# Confsion Matrix\n",
        "cm_knn = metrics.confusion_matrix(target_test, target_predicted_knn)\n",
        "cm_knn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "68cYt1O-DOsh",
        "colab": {}
      },
      "source": [
        "# Accuracy, Precision, Recall\n",
        "acc_knn = metrics.accuracy_score(target_test, target_predicted_knn)\n",
        "prec_knn = metrics.precision_score(target_test, target_predicted_knn)\n",
        "rec_knn = metrics.recall_score(target_test, target_predicted_knn)\n",
        "f1_knn = metrics.f1_score(target_test, target_predicted_knn)\n",
        "kappa_knn = metrics.cohen_kappa_score(target_test, target_predicted_knn)\n",
        "\n",
        "# Show Accuracy, Precision, Recall\n",
        "print('Accuracy:', acc_knn)\n",
        "print('Precision:', prec_knn)\n",
        "print('Recall:', rec_knn)\n",
        "print('F1 Score:', f1_knn)\n",
        "print('Cohens Kappa Score:', kappa_knn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIx2RNggUHwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Visualization Package\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Visualize ROC Curve\n",
        "target_predicted_knn_prob = knn.predict_proba(feature_test)[::,1]\n",
        "fp_rate_knn, tp_rate_knn, _ = metrics.roc_curve(target_test,  target_predicted_knn_prob)\n",
        "auc_knn = metrics.roc_auc_score(target_test, target_predicted_knn_prob)\n",
        "plt.plot(fp_rate_knn, tp_rate_knn, label=\"Decision Tree, auc=\"+str(auc_knn))\n",
        "plt.xlabel('false positive rate') \n",
        "plt.ylabel('true positive rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8lI6MVsODVbI"
      },
      "source": [
        "##### ***c. Naive Bayes Classifier***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLi-1y3AtMdT",
        "colab_type": "text"
      },
      "source": [
        "Naïve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSEgKSalpBal",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Naive Bayes Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u4vvv15IDVbK",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "\n",
        "# Modeling Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(feature_train, target_train)\n",
        "\n",
        "# Predict Test Data \n",
        "target_predicted_nb = nb.predict(feature_test)\n",
        "target_predicted_nb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwYLfr3tpEcu",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOX_Sc30DVbW",
        "colab": {}
      },
      "source": [
        "# Confsion Matrix\n",
        "cm_nb = metrics.confusion_matrix(target_test, target_predicted_nb)\n",
        "cm_nb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "unb5-oJWDVba",
        "colab": {}
      },
      "source": [
        "# Accuracy, Precision, Recall\n",
        "acc_nb = metrics.accuracy_score(target_test, target_predicted_nb)\n",
        "prec_nb = metrics.precision_score(target_test, target_predicted_nb)\n",
        "rec_nb = metrics.recall_score(target_test, target_predicted_nb)\n",
        "f1_nb = metrics.f1_score(target_test, target_predicted_nb)\n",
        "kappa_nb = metrics.cohen_kappa_score(target_test, target_predicted_nb)\n",
        "\n",
        "# Show Accuracy, Precision, Recall\n",
        "print('Accuracy:', acc_nb)\n",
        "print('Precision:', prec_nb)\n",
        "print('Recall:', rec_nb)\n",
        "print('F1 Score:', f1_nb)\n",
        "print('Cohens Kappa Score:', kappa_nb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5No0nWkUIzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Visualization Package\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Visualize ROC Curve\n",
        "target_predicted_nb_prob = nb.predict_proba(feature_test)[::,1]\n",
        "fp_rate_nb, tp_rate_nb, _ = metrics.roc_curve(target_test,  target_predicted_nb_prob)\n",
        "auc_nb = metrics.roc_auc_score(target_test, target_predicted_nb_prob)\n",
        "plt.plot(fp_rate_nb, tp_rate_nb, label=\"Decision Tree, auc=\"+str(auc_nb))\n",
        "plt.xlabel('false positive rate') \n",
        "plt.ylabel('true positive rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hj6oxGQw4_Zb"
      },
      "source": [
        "##### ***d. Model Evaluation Comparation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmrFcbYZQ-Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparing Model Performance\n",
        "print('Decision Tree Accuracy =',acc_dtree)\n",
        "print('Decision Tree Precision =',prec_dtree)\n",
        "print('Decision Tree Recall =',rec_dtree)\n",
        "print('Decision Tree F1-Score =', f1_dtree)\n",
        "print('_______________________')\n",
        "print('k-NN Accuracy =', acc_knn)\n",
        "print('k-NN Precision =', prec_knn)\n",
        "print('k-NN Recall =', rec_knn)\n",
        "print('k-NN F1-Score =', f1_knn)\n",
        "print('_______________________')\n",
        "print('Naive Bayes Accuracy =', acc_nb)\n",
        "print('Naive Bayes Precision =', prec_nb)\n",
        "print('Naive Bayes Recall =', rec_nb)\n",
        "print('Naive Bayes F1-Score =', f1_nb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttV_gOCTQ_Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparing ROC Curve\n",
        "plt.plot(fp_rate_dtree,tp_rate_dtree,label='Decision Tree, auc='+str(auc_dtree))\n",
        "plt.plot(fp_rate_knn,tp_rate_knn,label='K-NN, auc='+str(auc_knn))\n",
        "plt.plot(fp_rate_nb,tp_rate_nb,label='Naive Bayes, auc='+str(auc_nb))\n",
        "plt.title('ROC Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfAFxTVcd_LX",
        "colab_type": "text"
      },
      "source": [
        "## **Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjQnEb-5uC4l",
        "colab_type": "text"
      },
      "source": [
        "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBT3lLL9UeGl",
        "colab_type": "text"
      },
      "source": [
        "### **1. Clustering Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ArTSxmKuXBc",
        "colab_type": "text"
      },
      "source": [
        "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xOKLZILWP_lC"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sk-AHvbGP_lD",
        "colab": {}
      },
      "source": [
        "# Import Dataset\n",
        "df_income = pd.read_csv('https://raw.githubusercontent.com/rc-dbe/bigdatacertification/master/dataset/clustering.csv')\n",
        "df_income"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAs3kf2hP_lG",
        "colab": {}
      },
      "source": [
        "# Prints the Dataset Information\n",
        "df_income.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PGFHmTvTP_lK",
        "colab": {}
      },
      "source": [
        "# Prints Descriptive Statistics\n",
        "df_income.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cRaWmk38P_lO"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc0sRvtQ418Y",
        "colab_type": "text"
      },
      "source": [
        "First, we standardize the data to equalize the range and/or data variability. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DJLkVFMSP_lR",
        "colab": {}
      },
      "source": [
        "# Importing Standardscalar Module \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "# Set Name for StandardScaler as scaler\n",
        "scaler = StandardScaler() \n",
        "\n",
        "# Fit Standardization\n",
        "column_names = df_income.columns.tolist()\n",
        "df_income[column_names] = scaler.fit_transform(df_income[column_names])\n",
        "df_income.sort_index(inplace=True)\n",
        "df_income"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jKNSDZ0tP_lX",
        "colab": {}
      },
      "source": [
        "# Styling Plot\n",
        "sns.set() \n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "\n",
        "# Visualizing the Data\n",
        "sns.scatterplot(x='INCOME', y='SPEND', data=df_income)\n",
        "plt.title('Customer Segments')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Annual Spend')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lptWQ-EpWUk",
        "colab_type": "text"
      },
      "source": [
        "**Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5knTfWYeT3x",
        "colab_type": "text"
      },
      "source": [
        "##### ***a .K-Means Clustering***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-LzlK8QvPWt",
        "colab_type": "text"
      },
      "source": [
        "Kmeans algorithm is an iterative algorithm that tries to partition the dataset into Kpre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the inter-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unebu5ajef8R",
        "colab_type": "text"
      },
      "source": [
        "**Search for the Optimum Number of Clusters (k)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pwgQESo_n9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform Data Frame to Numpy Array\n",
        "income = df_income.to_numpy()\n",
        "income"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl---X3eeKx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elbow Method\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(1,11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(income)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "  \n",
        "# Visualize \n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('wcss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490H6p_AerBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Silhoutte Method\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "for n_cluster in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=n_cluster).fit(income)\n",
        "    label = kmeans.labels_\n",
        "    sil_coeff = silhouette_score(income, label, metric='euclidean')\n",
        "    print('For n_clusters={}, The Silhouette Coefficient is {}'.format(n_cluster, sil_coeff))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHgU53qpKmd",
        "colab_type": "text"
      },
      "source": [
        "**Modeling K-Means Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5c5fGHieyID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the K-Means Model to the Data\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "cluster = kmeans.fit_predict(income)\n",
        "\n",
        "# Visualising Clusters for k=3\n",
        "sns.scatterplot(x='INCOME', y='SPEND', data=df_income)\n",
        "plt.scatter(income[cluster == 0, 0], income[cluster == 0, 1], s = 50, label = 'Cluster 1')\n",
        "plt.scatter(income[cluster == 1, 0], income[cluster == 1, 1], s = 50, label = 'Cluster 2')\n",
        "plt.scatter(income[cluster == 2, 0], income[cluster == 2, 1], s = 50, label = 'Cluster 3')\n",
        "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],s=200,marker='s', alpha=0.7, label='Centroids')\n",
        "plt.title('Customer segments')\n",
        "plt.xlabel('Annual income')\n",
        "plt.ylabel('Annual spend')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fs66_EyfQcq",
        "colab_type": "text"
      },
      "source": [
        "##### ***b. Hierarchical Clustering***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs2lOj8Zv1kg",
        "colab_type": "text"
      },
      "source": [
        "Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzb0tZmIpQH2",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Hierarchical Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx3xvztZfPpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modeling and Visualizing Clusters by Dendogram\n",
        "import scipy.cluster.hierarchy as sch\n",
        "dend = sch.dendrogram(sch.linkage(income, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customer')\n",
        "plt.ylabel('Euclidean')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4JhA2qIfmBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the Hierarchical Clustering Model to the Dataset\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
        "hcluster = hc.fit_predict(income)\n",
        "\n",
        "# Visualising Clusters for k=3\n",
        "sns.scatterplot(x='INCOME', y='SPEND', data=df_income)\n",
        "plt.scatter(income[hcluster == 0, 0], income[hcluster == 0, 1], s = 50, label = 'Cluster 1')\n",
        "plt.scatter(income[hcluster == 1, 0], income[hcluster == 1, 1], s = 50, label = 'Cluster 2')\n",
        "plt.scatter(income[hcluster == 2, 0], income[hcluster == 2, 1], s = 50, label = 'Cluster 3')\n",
        "plt.title('Clusters of customers')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Annual Spend')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJYlm7FMgdrr",
        "colab_type": "text"
      },
      "source": [
        "### **4. Association Rule Mode**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUjS3bK7wRDh",
        "colab_type": "text"
      },
      "source": [
        "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unphJxPcqJgM",
        "colab_type": "text"
      },
      "source": [
        "#### ***a. Market Basket Analysis***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otw_vACrwgWI",
        "colab_type": "text"
      },
      "source": [
        "Market Basket Analysis is a modelling technique based upon the theory that if you buy a certain group of items, you are more (or less) likely to buy another group of items. For example, if you are in an English pub and you buy a pint of beer and don't buy a bar meal, you are more likely to buy crisps at the same time than somebody who didn't buy beer.\n",
        "\n",
        "The set of items a customer buys is referred to as an itemset, and market basket analysis seeks to find relationships between purchases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TldLIj98nNV_",
        "colab_type": "text"
      },
      "source": [
        "**Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWZzEV3wnOvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dataset\n",
        "df_retail = pd.read_excel(\"https://github.com/rc-dbe/bigdatacertification/blob/master/dataset/Online%20Retail.xlsx?raw=true\")\n",
        "df_retail"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekFovn-WneZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints the Dataset Information\n",
        "df_retail.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBKPMonSngIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints Descriptive Statistics\n",
        "df_retail.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHFt3_m6nrWp",
        "colab_type": "text"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th7obKNKn1Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove Additional Spaces\n",
        "df_retail['Description'] = df_retail['Description'].str.strip()\n",
        "\n",
        "# Remove Missing Values\n",
        "df_retail.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\n",
        "\n",
        "# Remove Cancelled Orders\n",
        "df_retail['InvoiceNo'] = df_retail['InvoiceNo'].astype('str')\n",
        "df_retail = df_retail[~df_retail['InvoiceNo'].str.contains('C')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwU8CXkToAEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Encode Function\n",
        "def encode_units(x):\n",
        "    if x <= 0:\n",
        "        return 0\n",
        "    if x >= 1:\n",
        "        return 1\n",
        "\n",
        "def create_basket(country_filter):\n",
        "    basket = (df_retail[df_retail['Country'] == country_filter]\n",
        "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
        "          .sum().unstack().reset_index().fillna(0)\n",
        "          .set_index('InvoiceNo'))\n",
        "    return basket"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5aW_55boX0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Retail Data by Country : France\n",
        "country_filter = 'France'\n",
        "basket_french = create_basket('France')\n",
        "basket_sets = basket_french.applymap(encode_units)\n",
        "basket_sets.drop('POSTAGE', inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSxgR5qSolR5",
        "colab_type": "text"
      },
      "source": [
        "**Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM-nfTzrpZxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Apriori Module\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "# Search for Frequent Itemsets\n",
        "frequent_itemsets = apriori(basket_sets, min_support=0.05, use_colnames=True)\n",
        "frequent_itemsets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9__Y6ycpdcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Association Rules Module\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "# Generate Rules\n",
        "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.2)\n",
        "rules.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odVD7uBxpj3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorting the Rules by Confidence\n",
        "rules.sort_values(['confidence'], axis=0, \n",
        "                 ascending=False, inplace=True) \n",
        "rules"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD1eXMch4DHa",
        "colab_type": "text"
      },
      "source": [
        "### **5. Dimensionality Reduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvtoFF22xWk4",
        "colab_type": "text"
      },
      "source": [
        "Dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmqOjux3YK45",
        "colab_type": "text"
      },
      "source": [
        "#### ***a. Principal Component Analysis***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDYlavkQxfu7",
        "colab_type": "text"
      },
      "source": [
        "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARnsFi-WWwY9",
        "colab_type": "text"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGSFubLKWs2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here We Are Using Inbuilt Dataset of Scikit-Learn \n",
        "from sklearn.datasets import load_breast_cancer \n",
        "  \n",
        "# Instantiating \n",
        "cancer = load_breast_cancer() \n",
        "\n",
        "# Creating Dataframe \n",
        "df_pca = pd.DataFrame(cancer['data'], columns = cancer['feature_names']) \n",
        "df_pca"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gnddmOxXn3b",
        "colab_type": "text"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZMG7vi0XqUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Standardscalar Module \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "# Set Name for StandardScaler as scaler\n",
        "scaler = StandardScaler() \n",
        "\n",
        "# Fit Standardization\n",
        "scaler.fit(df_pca) \n",
        "\n",
        "# Transformed Data\n",
        "df_pca_scaled = pd.DataFrame(scaler.transform(df_pca))\n",
        "\n",
        "# Checking Data\n",
        "df_pca_scaled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5_tiYsXzeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit Standardization\n",
        "scaler.fit(df_pca) \n",
        "\n",
        "# Transformed Data\n",
        "df_pca_scaled = pd.DataFrame(scaler.transform(df_pca))\n",
        "\n",
        "# Checking Head of Data\n",
        "df_pca_scaled.head(5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ysjghMfb7g",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Principal Component Analysis (2 Principal Components)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBz9xAgLfj3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing PCA Module\n",
        "from sklearn.decomposition import PCA \n",
        "\n",
        "# Modeling PCA with Components = 2 \n",
        "pca2 = PCA(n_components = 2) \n",
        "\n",
        "# Apply Model to Data\n",
        "pca2.fit(df_pca_scaled) \n",
        "\n",
        "# Show Result\n",
        "x_pca2 = pca2.transform(df_pca_scaled) \n",
        "df_pca2_result = pd.DataFrame(x_pca2)\n",
        "df_pca2_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JscmyNkEgWk7",
        "colab_type": "text"
      },
      "source": [
        "Visualizing Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8CEOddQgaHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot \n",
        "plt.scatter(x_pca2[:, 0], x_pca2[:, 1], c = cancer['target'], cmap ='plasma') \n",
        "\n",
        "# Labeling X and Y Axes \n",
        "plt.title('Principal Component Analysis')\n",
        "plt.xlabel('First Principal Component') \n",
        "plt.ylabel('Second Principal Component')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFYap0MguLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting Heatmap\n",
        "df_comp2 = pd.DataFrame(pca2.components_, columns = cancer['feature_names']) \n",
        "sns.heatmap(df_comp2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLU-K9Ji0LHU",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Principal Component Analysis (3 Principal Components)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25yJFISS0cSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing PCA Module\n",
        "from sklearn.decomposition import PCA \n",
        "\n",
        "# Modeling PCA with Components = 3\n",
        "pca3 = PCA(n_components = 3) \n",
        "\n",
        "# Apply Model to Data\n",
        "pca3.fit(df_pca_scaled) \n",
        "\n",
        "# Show Result\n",
        "x_pca3 = pca3.transform(df_pca_scaled) \n",
        "df_pca3_result = pd.DataFrame(x_pca3)\n",
        "df_pca3_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkwUSvvb0q1K",
        "colab_type": "text"
      },
      "source": [
        "Visualizing Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2W58UF70sr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Axes3D Module\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot \n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(x_pca3[:, 0], x_pca3[:, 1], x_pca3[:, 2], c = cancer['target'], cmap ='plasma') \n",
        "\n",
        "# Labeling X and Y Axes \n",
        "plt.title('Principal Component Analysis')\n",
        "ax.set_xlabel('First Principal Component') \n",
        "ax.set_ylabel('Second Principal Component')\n",
        "ax.set_zlabel('Third Principal Component')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MefBVTflDv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting Heatmap\n",
        "df_comp3 = pd.DataFrame(pca3.components_, columns = cancer['feature_names']) \n",
        "sns.heatmap(df_comp3)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}